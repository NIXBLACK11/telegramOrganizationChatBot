{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "miU-iTTABisp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa2dec9d-e06f-46ae-9f32-5c4abf3a0e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting laser_encoders\n",
            "  Downloading laser_encoders-0.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting sacremoses==0.1.0 (from laser_encoders)\n",
            "  Downloading sacremoses-0.1.0-py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.1/895.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unicategories>=0.1.2 (from laser_encoders)\n",
            "  Downloading unicategories-0.1.2.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from laser_encoders) (0.1.99)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from laser_encoders) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from laser_encoders) (2.1.0+cu121)\n",
            "Collecting fairseq>=0.12.2 (from laser_encoders)\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses==0.1.0->laser_encoders) (4.66.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.12.2->laser_encoders) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.12.2->laser_encoders) (3.0.8)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitarray (from fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq>=0.12.2->laser_encoders) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->laser_encoders) (2.1.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from unicategories>=0.1.2->laser_encoders) (1.4.4)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq>=0.12.2->laser_encoders) (6.0.1)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq>=0.12.2->laser_encoders) (4.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq>=0.12.2->laser_encoders) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->laser_encoders) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->laser_encoders) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, unicategories, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291809 sha256=25ceaaba018cc319f79626efa989583e9fbc355fc1ff47a8aa6cef3efa46d126\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for unicategories (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicategories: filename=unicategories-0.1.2-py2.py3-none-any.whl size=30843 sha256=c72881134fc8c62e4b735d64c28f4e481e51d225626c7e619cd56487eed1aa3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/6d/14/7135674b9daa3996f7f0d9bc1ccff5b7d50d6f1c4a16dc7d90\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=9a150f9283dcb9c67495fe5d0a34290aff36d7144487dba3a7dd9a15baf112e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq unicategories antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, unicategories, sacremoses, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq, laser_encoders\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 laser_encoders-0.0.1 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0 sacremoses-0.1.0 unicategories-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install laser_encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/NIXBLACK11/telegramOrganizationChatBot/main/v2/intents.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNiDaqoKoSPP",
        "outputId": "7564e4ac-c631-4e89-85c9-9cd895a3c9f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-08 12:34:58--  https://raw.githubusercontent.com/NIXBLACK11/telegramOrganizationChatBot/main/v2/intents.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15941 (16K) [text/plain]\n",
            "Saving to: ‘intents.json’\n",
            "\n",
            "\rintents.json          0%[                    ]       0  --.-KB/s               \rintents.json        100%[===================>]  15.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-02-08 12:34:58 (89.8 MB/s) - ‘intents.json’ saved [15941/15941]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import json"
      ],
      "metadata": {
        "id": "MpOwZ-ZbB12G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from laser_encoders import LaserEncoderPipeline"
      ],
      "metadata": {
        "id": "8dCenhvTDQeF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LaserEncoderPipeline(lang=\"eng_Latn\")"
      ],
      "metadata": {
        "id": "8NSEBbmUDwZc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b027ff1-44f1-4733-8837-b7637e93bbd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.01M/1.01M [00:00<00:00, 31.4MB/s]\n",
            "100%|██████████| 179M/179M [00:03<00:00, 58.7MB/s]\n",
            "100%|██████████| 470k/470k [00:00<00:00, 24.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"CUDA is available! Using GPU.\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"CUDA is not available. Using CPU.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TC4n1Qp7dDa8",
        "outputId": "26d76bcf-dfeb-4177-b390-7dc9ab11af3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available! Using GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []"
      ],
      "metadata": {
        "id": "xzmBp0d0B3Q0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the intents JSON file\n",
        "with open('intents.json') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "UMmSl5IhB4kG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = encoder.encode_sentences([\"How are you\"])[0]\n",
        "print(test_sentence)\n",
        "print(len(test_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpfl8XYqOFRS",
        "outputId": "2cc9c677-2e9b-41cd-ea6d-1b1ac55eb5b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.00115137 -0.00047817 -0.00038098 ... -0.00121862  0.0027636\n",
            "  0.00175885]\n",
            "1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        processed_sentence = ''.join(char.lower() for char in pattern if char.isalnum() or char.isspace())\n",
        "        tokenized_words = encoder.encode_sentences([pattern])[0]\n",
        "        docs_x.append(tokenized_words)\n",
        "        docs_y.append(intent['tag'])\n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])"
      ],
      "metadata": {
        "id": "kD9bNMMsB6Gi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(labels)"
      ],
      "metadata": {
        "id": "XmVXBw0eB8AX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "output = []"
      ],
      "metadata": {
        "id": "FK-1l3VdC7sJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, doc in enumerate(docs_x):\n",
        "    output_row = [0] * len(labels)\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "    output.append(output_row)"
      ],
      "metadata": {
        "id": "rCIu6FOGCETx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = np.array(docs_x)\n",
        "output = np.array(output)"
      ],
      "metadata": {
        "id": "wSCXnbV9CFtX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, training, output):\n",
        "        self.training = torch.tensor(training, dtype=torch.float32)\n",
        "        self.output = torch.tensor(output, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.training[idx], self.output[idx]\n",
        "\n",
        "# Assuming you have 'training' and 'output' from your data processing code\n",
        "# Instantiate the dataset\n",
        "dataset = ChatbotDataset(training=training, output=output)\n",
        "\n",
        "# Instantiate the DataLoader\n",
        "batch_size = 32\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "VvMdsPo4IRlE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model using PyTorch\n",
        "class ChatbotModel(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(ChatbotModel, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, output_size)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.l1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.l2(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.l3(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "LCDkzVr4CI77"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(training[0])\n",
        "hidden_size = 512\n",
        "output_size = len(output[0])\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "YTNxX0IJCKZQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = ChatbotModel(input_size, hidden_size, output_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "6Spz9ZBkCMRu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_outputs in dataloader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_outputs = batch_outputs.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(batch_inputs)\n",
        "\n",
        "            # Get predictions\n",
        "            _, predicted_labels = torch.max(outputs, 1)\n",
        "            batch_outputs = torch.argmax(batch_outputs, dim=-1)\n",
        "\n",
        "            # Count correct predictions\n",
        "            correct_predictions += (predicted_labels == batch_outputs).sum().item()\n",
        "            total_samples += batch_outputs.size(0)\n",
        "\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    model.train()  # Set the model back to training mode\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "6Mgg5WlIk0gp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_outputs in train_loader:\n",
        "\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_outputs = batch_outputs.to(device)\n",
        "        # Zero the gradientsNeuralNetwork\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_outputs)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "      train_accuracy = calculate_accuracy(model, train_loader, device)\n",
        "      print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}, Training Accuracy: {train_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5JwCzyoCXLU",
        "outputId": "c013f6e0-b0a7-4e93-f73a-ecfab3abb64c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/10, Loss: 3.2865238189697266, Training Accuracy: 4.79%\n",
            "Epoch 1/10, Loss: 3.4090986251831055, Training Accuracy: 17.47%\n",
            "Epoch 2/10, Loss: 3.2634243965148926, Training Accuracy: 22.60%\n",
            "Epoch 3/10, Loss: 2.8771555423736572, Training Accuracy: 27.40%\n",
            "Epoch 4/10, Loss: 2.318315267562866, Training Accuracy: 41.78%\n",
            "Epoch 5/10, Loss: 2.406074047088623, Training Accuracy: 49.32%\n",
            "Epoch 6/10, Loss: 1.9173351526260376, Training Accuracy: 63.36%\n",
            "Epoch 7/10, Loss: 2.004138231277466, Training Accuracy: 60.62%\n",
            "Epoch 8/10, Loss: 1.7296844720840454, Training Accuracy: 58.90%\n",
            "Epoch 9/10, Loss: 1.1002148389816284, Training Accuracy: 63.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "4fbwaq-QCYmh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load('model.pth')\n",
        "model = ChatbotModel(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTXOx07KiK4S",
        "outputId": "c991df17-0305-42ce-ccf1-c4498ba76512"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatbotModel(\n",
              "  (l1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (l3): Linear(in_features=512, out_features=27, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0d6x6vwieXZ",
        "outputId": "784d7c15-7469-485f-b221-0b9c6255c28d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatbotModel(\n",
              "  (l1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
              "  (l3): Linear(in_features=512, out_features=27, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_input(user_input):\n",
        "    # tokenized_input = nltk.word_tokenize(user_input)\n",
        "    # tokenized_input = [stemmer.stem(word.lower()) for word in tokenized_input]\n",
        "    user_input = user_input.lower()\n",
        "    input_bag = encoder.encode_sentences([user_input])[0]\n",
        "\n",
        "    input_bag = torch.tensor(input_bag, dtype=torch.float32).to(device)\n",
        "    input_bag = input_bag.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "    results = model(input_bag)\n",
        "    results_index = torch.argmax(results).item()\n",
        "    tag = labels[results_index]\n",
        "\n",
        "    for intent in data['intents']:\n",
        "        if intent['tag'] == tag:\n",
        "            return random.choice(intent['responses'])"
      ],
      "metadata": {
        "id": "1aMNV4OxCaYv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_input(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b79l81N2h-ZJ",
        "outputId": "127de670-045b-4cac-9222-3d160dc5da1e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Talk to you later'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Hi\", \"who made you\", \"what are the bank timings\", \"what is my account balance\", \"where is the bank located\",\n",
        "              \"नमस्ते\", \"आपको किसने बन\", \"बैंक का समय क्या\", \"मेरे खाते का शेष राशि क्\", \"बैंक कहाँ स्थित है?\",\n",
        "              \"Hola\", \"¿Quién te creó?\", \"¿Cuáles son los horarios del banco?\", \"¿Cuál es el saldo de mi cuenta?\", \"¿Dónde está ubicado el banco?\",\n",
        "              \"what documents are required to open an account\", \"खाता खोलने के लिए कौन-कौन से दस्तावेज़ आ\", \"¿Qué documentos se requieren para abrir una cuenta?\"]\n",
        "\n",
        "for sentence in sentences:\n",
        "  print(f\"You: {sentence}\")\n",
        "  print(f\"Bot: {process_input(sentence)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DogzyycsM4t3",
        "outputId": "220cbc54-79db-4556-c34a-2d1903487281"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: Hi\n",
            "Bot: Sad to see you go :(\n",
            "You: who made you\n",
            "Bot: My creator is NIXBLACK\n",
            "You: what are the bank timings\n",
            "Bot: Our bank follows a holiday schedule provided in advance. For details, refer to the official bank calendar or contact our customer service for information about upcoming holidays.\n",
            "You: what is my account balance\n",
            "Bot: balance_enquiry\n",
            "You: where is the bank located\n",
            "Bot: The bank is located at Nainital, Uttarakhand\n",
            "You: नमस्ते\n",
            "Bot: Come back soon\n",
            "You: आपको किसने बन\n",
            "Bot: I am your Chatbanker.\n",
            "You: बैंक का समय क्या\n",
            "Bot: The bank is open from 9am to 5pm Monday-Friday and from 9am to 12pm on Saturdays!\n",
            "You: मेरे खाते का शेष राशि क्\n",
            "Bot: Our bank follows a holiday schedule provided in advance. For details, refer to the official bank calendar or contact our customer service for information about upcoming holidays.\n",
            "You: बैंक कहाँ स्थित है?\n",
            "Bot: The bank is located at Nainital, Uttarakhand\n",
            "You: Hola\n",
            "Bot: Sad to see you go :(\n",
            "You: ¿Quién te creó?\n",
            "Bot: My creator is NIXBLACK\n",
            "You: ¿Cuáles son los horarios del banco?\n",
            "Bot: Our bank follows a holiday schedule provided in advance. For details, refer to the official bank calendar or contact our customer service for information about upcoming holidays.\n",
            "You: ¿Cuál es el saldo de mi cuenta?\n",
            "Bot: balance_enquiry\n",
            "You: ¿Dónde está ubicado el banco?\n",
            "Bot: The bank is located at Nainital, Uttarakhand\n",
            "You: what documents are required to open an account\n",
            "Bot: You can bring your pancard, aadhar card\n",
            "You: खाता खोलने के लिए कौन-कौन से दस्तावेज़ आ\n",
            "Bot: You can bring your pancard, aadhar card\n",
            "You: ¿Qué documentos se requieren para abrir una cuenta?\n",
            "Bot: You can bring your pancard, aadhar card\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Bank Chatbot - Ask me anything!\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    response = process_input(user_input)\n",
        "    print(\"Bot: \" + response)"
      ],
      "metadata": {
        "id": "MPG4RuNMZf7F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "outputId": "b3322419-0730-4127-d7a9-c4de1f2a332b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bank Chatbot - Ask me anything!\n",
            "You: hello\n",
            "Bot: Sad to see you go :(\n",
            "You: what\n",
            "Bot: Sad to see you go :(\n",
            "You: eejdsknscd\n",
            "Bot: You can contact the bank at: BANK_PHONE_NUMBER\n",
            "You: tell me the account balance\n",
            "Bot: balance_enquiry\n",
            "You: hello\n",
            "Bot: Come back soon\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-c6fcbdf23efd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bank Chatbot - Ask me anything!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "input_size = len(training[0])\n",
        "hidden_size = 512\n",
        "output_size = len(output[0])\n",
        "\n",
        "sizes = [input_size, output_size, hidden_size]\n",
        "\n",
        "with open('sizes.pkl', 'wb') as file:\n",
        "    loaded_array = pickle.dump(sizes, file)\n",
        "\n",
        "\n",
        "with open('lables.pkl', 'wb') as file:\n",
        "    loaded_array = pickle.dump(labels, file)"
      ],
      "metadata": {
        "id": "nQjzc-pWkg0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah"
      ],
      "metadata": {
        "id": "OjJvbZVVmDch"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}